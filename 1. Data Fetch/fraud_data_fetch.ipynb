{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1 : Data Fetch: Import Dependencies, Read CSV and explore Data features  \n",
    "\n",
    "- First section is set out to import all required dependencies and to read raw_csv file into dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ IN CSV as raw_csv\n",
    "raw_csv = pd.read_csv(\"Resources/PS_20174392719_1491204439457_log.csv\")\n",
    "raw_csv.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring existing data set:\n",
    "The following section explores the raw data `raw_csv` to identify cleaning required .\n",
    "- In this section we review the existing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore csv info\n",
    "raw_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Summary of isFraud Type \n",
    "type_summary = pd.crosstab(raw_csv['type'], raw_csv['isFraud'])\n",
    "type_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore unique variables\n",
    "unique_counts=raw_csv.nunique()\n",
    "print(unique_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a total of 6,362,620 rows and 8,213 instances of Fraud which is 0.129%.  \n",
    "We have identified through column 'type' that Fraud only occured in 2 transaction types : 'CASH_OUT' and 'TRANSFER'.  \n",
    "All other Types should be dropped from the model as they do not appear to have fraud in this case. \n",
    "\n",
    "There are 743 Steps as hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore  Distribution of numerical data.\n",
    "distribution_summary  = raw_csv.describe()\n",
    "print(distribution_summary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Distribution of Categorical Data\n",
    "# Generate the distribution summary for categorical columns\n",
    "categorical_summary = {}\n",
    "for col in raw_csv.select_dtypes(include=['object']):\n",
    "    categorical_summary[col] = raw_csv[col].value_counts()\n",
    "\n",
    "# Display the distribution summary for categorical columns\n",
    "for col, counts in categorical_summary.items():\n",
    "    print(f\"\\nDistribution for column '{col}':\")\n",
    "    print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore if Null exists\n",
    "raw_csv.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of 0 in 'oldbalanceORG'\n",
    "missing_oldbalance = (raw_csv['oldbalanceOrg'] == 0).sum()\n",
    "\n",
    "# Print the result\n",
    "print(missing_oldbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of 0 in 'amount'\n",
    "missing_transaction = (raw_csv['amount'] == 0).sum()\n",
    "\n",
    "# Print the result\n",
    "print(missing_transaction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Data Clean and Create New Features & Digitise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy raw_csv and use clean_csv to clean up and add features\n",
    "clean_csv_df = raw_csv.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows where the 'type' is 'Cash_IN', 'Debit', or 'Payment' in-place\n",
    "types_to_drop = ['CASH_IN', 'DEBIT', 'PAYMENT']\n",
    "clean_csv_df.drop(clean_csv_df[clean_csv_df['type'].isin(types_to_drop)].index, inplace=True)\n",
    "clean_csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'amount' is equal to 0\n",
    "clean_csv_df = clean_csv_df[clean_csv_df['amount'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 values in 'oldbalanceORG' column with values from 'amount' column\n",
    "clean_csv_df.loc[clean_csv_df['oldbalanceOrg'] == 0, 'oldbalanceOrg'] = clean_csv_df.loc[clean_csv_df['oldbalanceOrg'] == 0, 'amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reset index and view data infom\n",
    "clean_csv_df.reset_index(drop=True, inplace=True)\n",
    "clean_csv_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW FEATURES based on Step : Hour  Cycle and Week\n",
    "\n",
    "## Create hour cycle to reset to 1 every 24 steps\n",
    "clean_csv_df['hour'] = ((clean_csv_df['step'] - 1) % 24) + 1\n",
    "\n",
    "## Create Cycle every 24 Hours\n",
    "# Create a new column 'new_column' initialized with 1\n",
    "clean_csv_df['day'] = 1\n",
    "\n",
    "# Calculate the increments of 1 for every 24 increases in the 'step' column\n",
    "clean_csv_df['day'] += (clean_csv_df['step'] - 1) // 24\n",
    "\n",
    "\n",
    "# Create a new column 'week' by increasing 1 for each cycle\n",
    "clean_csv_df['week'] = ((clean_csv_df['step'] - 1) // 168) + 1\n",
    "\n",
    "# Drop the intermediate 'cycles' column if needed\n",
    "# raw_csv.drop(columns=['cycles'], inplace=True)\n",
    "\n",
    "clean_csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW FEATURES based on old and new balance\n",
    "\n",
    "# Create new columns for the division of 'amount' by 'oldbalanceOrg'\n",
    "clean_csv_df['trans_weight'] = clean_csv_df['amount'] / clean_csv_df['oldbalanceOrg']\n",
    "\n",
    "# % Change in Balance Transaction \n",
    "clean_csv_df['bal_change_per'] = (clean_csv_df['newbalanceOrig']-   clean_csv_df['oldbalanceOrg'])/ clean_csv_df['oldbalanceOrg']\n",
    "\n",
    "\n",
    "# Create Large Transaction Figure\n",
    "clean_csv_df['large_transaction'] = (clean_csv_df['amount'] > 100000).astype(int)\n",
    "\n",
    "\n",
    "# Round 'trans_weight' to the nearest two decimals\n",
    "clean_csv_df['trans_weight'] = clean_csv_df['trans_weight'].round(2)\n",
    "\n",
    "# Round 'bal_change_per' to the nearest two decimals\n",
    "clean_csv_df['bal_change_per'] = clean_csv_df['bal_change_per'].round(2)\n",
    "\n",
    "\n",
    "clean_csv_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW FEATURES Creating Dummies bases on 'type'\n",
    "# Create dummy variables for the 'type' column\n",
    "dummy_columns = pd.get_dummies(clean_csv_df['type'], prefix='type')\n",
    "\n",
    "# Concatenate the dummy variables with the original DataFrame\n",
    "clean_csv_df = pd.concat([clean_csv_df, dummy_columns], axis=1)\n",
    "\n",
    "# Display the updated DataFrame with the dummy columns\n",
    "clean_csv_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "############# fraud_det_df\n",
    "#########################\n",
    "## PRINT the cleaned data to csv fraud_det_df\n",
    "fraud_det_df = clean_csv_df.copy()\n",
    "\n",
    "# Export the DataFrame as a CSV file. \n",
    "# fraud_det_df.to_csv(\"Resources/fraud_det_df.csv\", encoding='utf8', index=False)\n",
    "fraud_det_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and confirm column features\n",
    "fraud_det_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "## Copy dataframe to convert data for digital transformation\n",
    "fraud_det_dig_df = fraud_det_df.copy()\n",
    "fraud_det_dig_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'nameOrig' and 'type' columns\n",
    "fraud_det_dig_df.drop(columns=['nameOrig', 'type'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Digitize 'nameDest' based on categories and create the 'Destination' column\n",
    "fraud_det_dig_df['Destination'] = pd.factorize(fraud_det_dig_df['nameDest'])[0]\n",
    "\n",
    "# Print the updated DataFrame to check the result\n",
    "fraud_det_dig_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nameDest as has been factorised\n",
    "fraud_det_dig_df.drop(columns=['nameDest'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "############# fraud_det_dig_df\n",
    "#########################\n",
    "# View summary of digitised dataframe for model\n",
    "fraud_det_dig_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3 : Data Summary and Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate that no Null values have carried through.\n",
    "null_values_count = fraud_det_df.isnull().sum()\n",
    "null_values_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Summary of isFraud Type \n",
    "type_summary = pd.crosstab(fraud_det_df['type'], fraud_det_df['isFraud'])\n",
    "type_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Summary of isFraud Type \n",
    "type_summary = pd.crosstab(fraud_det_df['type'], fraud_det_df['isFlaggedFraud'])\n",
    "type_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Summary of isFraud hour \n",
    "hour_summary = pd.crosstab(fraud_det_df['hour'], fraud_det_df['isFraud'])\n",
    "hour_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show Summary of isFraud hour \n",
    "week_summary = pd.crosstab(fraud_det_df['week'], fraud_det_df['isFraud'])\n",
    "week_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of 'fraud_status' with labels and y-axis in millions\n",
    "# Plot the distribution of 'fraud_status' with labels and y-axis in millions\n",
    "plt.figure(figsize=(8, 6))  # Adjust the figure size if needed\n",
    "ax = sns.countplot(data=fraud_det_df, x='isFraud')\n",
    "\n",
    "# Add labels to the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():,}\", (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Distribution of Fraudulent Transactions\")\n",
    "plt.xlabel(\"Fraud Status\")\n",
    "plt.ylabel(\"Count (Millions)\")\n",
    "plt.show()\n",
    "\n",
    "##\n",
    "count_fraud = fraud_det_df['isFraud'].value_counts()\n",
    "print(count_fraud)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis and EDA and Fraud detection\n",
    "\n",
    "The provided data has the financial transaction data as well as the target variable isFraud, which is the actual fraud status of the transaction and isFlaggedFraud is the indicator which the simulation is used to flag the transaction using some threshold.\n",
    "\n",
    "The goal should be how we can improve and come up with better threshold to capture the fraud transaction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualization, we took a smaller random sample of 1000 transactions from the original dataset to make it easier to visualize. Then, we created three side-by-side histograms, one for each numeric feature ('amount', 'oldbalanceOrg', 'newbalanceOrig'), showing the distribution of these values to help understand how the data is spread out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the numerical features in your dataset\n",
    "numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
    "sampled_data = fraud_det_df.sample(n=1000)  \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(1, len(numeric_features), i)\n",
    "    sns.histplot(data=sampled_data, x=feature, kde=True)\n",
    "    plt.title(f\"Distribution of {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis of Categorical Features and 'isFraud'\n",
    "categorical_features = ['type', 'hour', 'day', 'week']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=fraud_det_df, x=feature, hue='isFraud')\n",
    "    plt.title(f\"Fraud Distribution by {feature}\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend(title='Fraud Status', labels=['Not Fraud', 'Fraud'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above visualizations helps us compare the occurrence of fraud between different categories of categorical features. By visually examining the distribution of fraudulent and non-fraudulent transactions, we can identify patterns and trends related to fraud occurrence in specific scenarios, such as transaction types, hours, days, or weeks. These insights enable fraud investigators to focus their efforts on high-risk categories and implement targeted fraud prevention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate Analysis of Numeric Features and 'isFraud'\n",
    "numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
    "\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=fraud_det_df, x='isFraud', y=feature)\n",
    "    plt.title(f\"{feature} vs. Fraud Status\")\n",
    "    plt.xlabel(\"Fraud Status\")\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use bivariate analysis using box plots provides valuable insights into the distribution of numeric features based on fraud status. This information helps in designing more effective fraud detection strategies and improving the accuracy of detecting fraudulent transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Outlier detection \n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Numeric features for outlier detection\n",
    "# numeric_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig']\n",
    "\n",
    "# # Create box plots for numeric features\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# for i, feature in enumerate(numeric_features, 1):\n",
    "#     plt.subplot(1, len(numeric_features), i)\n",
    "#     sns.boxplot(data=fraud_det_df, x=feature)\n",
    "#     plt.title(f\"Box Plot of {feature}\")\n",
    "#     plt.xlabel(feature)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4: Print CSV for Data Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print fraud_det_df which is first working data frame\n",
    "### Data is not digitised and still contains dummies\n",
    "\n",
    "# fraud_det_df.to_csv(\"Resources/fraud_det.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print fraud_det__dig_df which is first working data frame\n",
    "# ### Data is not digitised and still contains dummies\n",
    "# fraud_det_dig_df.to_csv(\"Resources/fraud_det_dig_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_det_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_det_dig_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5 : Addresing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"Resources/fraud_det_dig_df.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Visualize the distribution of 'isFraud' before balancing\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(data=raw_df, x='isFraud')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():,}\", (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Distribution of Fraudulent Transactions (Before Balancing)\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count (Millions)\")\n",
    "plt.show()\n",
    "\n",
    "## DEFINE target Class and set X variables\n",
    "# Separate features and target variable\n",
    "X = raw_df.drop('isFraud', axis=1)\n",
    "y = raw_df['isFraud']\n",
    "\n",
    "# Perform train-test split on the original DataFrame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "######\n",
    "## Variables to use for model\n",
    "######\n",
    "# Perform Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "##X_rus_test, y_rus_test = rus.fit_resample(X_test,y_test)\n",
    "\n",
    "######\n",
    "##\n",
    "######\n",
    "\n",
    "\n",
    "# Visualize the distribution of 'isFraud' after Random Undersampling\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x=y_rus)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():,}\", (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Distribution of Fraudulent Transactions (After Random Undersampling)\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Perform Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Visualize the distribution of 'isFraud' after Random Oversampling\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x=y_ros)\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():,}\", (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "plt.title(\"Distribution of Fraudulent Transactions (After Random Oversampling)\")\n",
    "plt.xlabel(\"isFraud\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Fetch for Tableau Visualisation\n",
    "Fetch Data for TABLEAU Visualisation - minimise and aggregate where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tableau_df = pd.read_csv(\"Resources/PS_20174392719_1491204439457_log.csv\")\n",
    "tableau_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate time and hour features\n",
    "tableau_df['hour'] = ((tableau_df['step'] - 1) % 24) + 1\n",
    "\n",
    "## Create Cycle every 24 Hours\n",
    "# Create a new column 'new_column' initialized with 1\n",
    "tableau_df['day'] = 1\n",
    "\n",
    "# Calculate the increments of 1 for every 24 increases in the 'step' column\n",
    "tableau_df['day'] += (tableau_df['step'] - 1) // 24\n",
    "\n",
    "# Create a new column 'week' by increasing 1 for each cycle\n",
    "tableau_df['week'] = ((tableau_df['step'] - 1) // 168) + 1\n",
    "\n",
    "tableau_df['time'] = (tableau_df['hour'] - 1).apply(lambda x: f\"{x % 12 + 1}:00 {'AM' if x < 12 else 'PM'}\")\n",
    "tableau_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dtop columns not to use for summary\n",
    "columns_to_drop = ['step', 'hour', 'isFlaggedFraud']\n",
    "tableau_df.drop(columns_to_drop, axis=1, inplace=True)\n",
    "tableau_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGFREGATE DATA SUM\n",
    "# Grouping by the specified columns and calculating the sum of 'amount' and 'oldbalanceOrg'\n",
    "agg_sum_df = tableau_df.groupby(['type', 'isFraud', 'time', 'day', 'week']).agg({\n",
    "    'amount': 'sum',\n",
    "    'oldbalanceOrg': 'sum',\n",
    "    'nameOrig': 'count'  # Calculates the count of 'nameOrig'\n",
    "}).reset_index()\n",
    "\n",
    "agg_sum_df.head()\n",
    "\n",
    "\n",
    "agg_sum_df.to_csv(\"Resources/tableau_agg_sum.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create specialised Dataframe summary  \n",
    "\n",
    "# Grouping by 'type' and 'isFraud' and aggregating the required statistics\n",
    "agg_unique_df = tableau_df.groupby(['type', 'isFraud']).agg({\n",
    "    'amount': ['mean', 'max', 'min'],\n",
    "    'nameDest': pd.Series.nunique,\n",
    "    'nameOrig': pd.Series.nunique  # Calculates the count of unique 'nameDest'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "agg_unique_df.columns = ['type', 'isFraud', 'average_amount', 'max_amount', 'min_amount', 'unique_recipient','unique_customer']\n",
    "agg_unique_df.to_csv(\"Resources/tableau_agg_unique.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
